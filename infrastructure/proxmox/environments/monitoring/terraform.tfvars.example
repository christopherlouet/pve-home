# =============================================================================
# Monitoring - Configuration Terraform
# =============================================================================
# PVE dedie au monitoring centralise de tous les environnements.
# 1. Copier ce fichier en terraform.tfvars
# 2. Remplir les valeurs avec vos informations
# 3. NE PAS commiter terraform.tfvars (contient des secrets)
# =============================================================================

# -----------------------------------------------------------------------------
# Connexion Proxmox (PVE dedie monitoring)
# -----------------------------------------------------------------------------

# URL du serveur Proxmox dedie au monitoring
proxmox_endpoint = "https://192.168.1.50:8006"

# Token API cree sur cette instance Proxmox
# Format: user@realm!token-name=secret
proxmox_api_token = "terraform@pve!terraform-token=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"

# true si certificat auto-signe (defaut Proxmox)
proxmox_insecure = true

# -----------------------------------------------------------------------------
# Infrastructure Proxmox
# -----------------------------------------------------------------------------

# Nom du node Proxmox dedie monitoring
default_node = "pve-mon"

# ID du template VM cloud-init
vm_template_id = 9000

# Datastore pour les disques
default_datastore = "local-lvm"

# -----------------------------------------------------------------------------
# Reseau
# -----------------------------------------------------------------------------

# Bridge reseau Proxmox
network_bridge = "vmbr0"

# Passerelle
network_gateway = "192.168.1.1"

# Serveurs DNS
network_dns = ["1.1.1.1", "8.8.8.8"]

# -----------------------------------------------------------------------------
# SSH
# -----------------------------------------------------------------------------

# Cle(s) SSH publique(s) pour acceder aux VMs
ssh_public_keys = [
  "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIxxx... user@workstation"
]

# -----------------------------------------------------------------------------
# Environnement
# -----------------------------------------------------------------------------

environment = "monitoring"

# -----------------------------------------------------------------------------
# Stack Monitoring (Prometheus + Grafana + Alertmanager)
# -----------------------------------------------------------------------------
# Supervise TOUS les PVE (prod, lab, monitoring)
# Les nodes Proxmox doivent etre accessibles depuis ce reseau
# -----------------------------------------------------------------------------

monitoring = {
  # Node Proxmox pour la VM monitoring (null = default_node)
  node = null

  # Configuration de la VM monitoring
  vm = {
    ip        = "192.168.1.51"
    cores     = 2
    memory    = 4096
    disk      = 30
    data_disk = 50 # Stockage metriques Prometheus
  }

  # TOUS les nodes Proxmox a monitorer (multi-PVE)
  # Chaque node a son propre token API (cree sur le PVE correspondant) :
  #   pveum user add prometheus@pve
  #   pveum aclmod / -user prometheus@pve -role PVEAuditor
  #   pveum user token add prometheus@pve prometheus -privsep=0
  proxmox_nodes = [
    { name = "pve-prod", ip = "192.168.1.100", token_value = "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx" },
    { name = "pve-lab", ip = "192.168.1.110", token_value = "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx" },
    { name = "pve-mon", ip = "192.168.1.50", token_value = "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx" },
  ]

  # Credentials pve-exporter (user et token_name communs a tous les nodes)
  pve_exporter = {
    user       = "prometheus@pve"
    token_name = "prometheus"
  }

  # Retention des metriques (jours)
  retention_days = 30

  # Mot de passe admin Grafana
  grafana_admin_password = "ChangeMePlease123!"

  # Notifications Telegram (optionnel)
  telegram = {
    enabled   = false
    bot_token = "" # Token du bot (@BotFather)
    chat_id   = "" # ID du chat/groupe
  }
}

# -----------------------------------------------------------------------------
# Minio S3 (Backend Terraform State)
# -----------------------------------------------------------------------------
# Stockage S3 auto-heberge pour l'etat Terraform.
# Le conteneur est deploye sur le PVE monitoring.
# Apres deploiement, configurer backend.tf dans chaque environnement.
# -----------------------------------------------------------------------------

minio = {
  ip               = "192.168.1.52"
  template_file_id = "local:vztmpl/debian-13-standard_13.1-2_amd64.tar.zst"
  cpu_cores        = 1
  memory_mb        = 512
  disk_size_gb     = 8
  data_disk_size_gb = 50
  root_user        = "minioadmin"
  root_password    = "${MINIO_ROOT_PASSWORD:?required}"
  port             = 9000
  console_port     = 9001
  buckets          = ["tfstate-prod", "tfstate-lab", "tfstate-monitoring"]
}

# -----------------------------------------------------------------------------
# Sauvegardes vzdump
# -----------------------------------------------------------------------------

backup = {
  enabled  = true
  schedule = "02:00"       # Quotidien a 2h du matin
  storage  = "local"       # Storage Proxmox pour les sauvegardes
  mode     = "snapshot"    # snapshot (pas d'arret), suspend, ou stop
  compress = "zstd"        # zstd (recommande), lzo, gzip, none
  retention = {
    keep_daily   = 7       # Garder 7 sauvegardes quotidiennes
    keep_weekly  = 0       # Pas de retention hebdomadaire
    keep_monthly = 0       # Pas de retention mensuelle
  }
}

# -----------------------------------------------------------------------------
# Cibles distantes (VMs sur d'autres PVE)
# -----------------------------------------------------------------------------
# VMs avec node_exporter sur d'autres PVE.
# Toutes les VMs sont sur le meme reseau 192.168.1.0/24.
# -----------------------------------------------------------------------------

remote_targets = [
  # VMs sur PVE prod (192.168.1.0/24)
  # {
  #   name = "web-server"
  #   ip   = "192.168.1.101"
  #   port = 9100
  #   labels = {
  #     app         = "web-server"
  #     type        = "vm"
  #     environment = "prod"
  #   }
  # },
  # VMs sur PVE lab (192.168.1.0/24)
  # {
  #   name = "file-server"
  #   ip   = "192.168.1.111"
  #   port = 9100
  #   labels = {
  #     app         = "file-server"
  #     type        = "vm"
  #     environment = "lab"
  #   }
  # },
]

# -----------------------------------------------------------------------------
# Custom Prometheus Scrape Configs (optionnel)
# -----------------------------------------------------------------------------
# Pour les configs avancees (relabel_configs, blackbox, metrics_path, params)
# Utiliser YAML inline avec heredoc:
#
# custom_scrape_configs = <<-YAML
#   - job_name: 'my-app-node'
#     static_configs:
#       - targets: ['192.168.1.101:9100']
#         labels:
#           app: 'my-app'
#     relabel_configs:
#       - source_labels: [__address__]
#         target_label: instance
#         replacement: 'my-app'
#
#   - job_name: 'my-app-blackbox'
#     metrics_path: /probe
#     params:
#       module: [http_2xx]
#     static_configs:
#       - targets: ['http://192.168.1.101/health']
#     relabel_configs:
#       - source_labels: [__address__]
#         target_label: __param_target
#       - target_label: __address__
#         replacement: '192.168.1.101:9115'
# YAML
# -----------------------------------------------------------------------------

custom_scrape_configs = ""

# -----------------------------------------------------------------------------
# Stack Tooling (Step-ca PKI + Harbor Registry + Authentik SSO)
# -----------------------------------------------------------------------------
# Services internes partages: PKI interne, registre Docker, SSO.
# Tous les services sont exposes via Traefik avec HTTPS auto.
# Domaine recommande: *.home.arpa (RFC 8375 pour homelab)
# -----------------------------------------------------------------------------

tooling = {
  # Activer/desactiver toute la stack tooling
  enabled = false

  # Node Proxmox pour la VM tooling (null = default_node)
  node = null

  # Configuration de la VM tooling
  # Recommandations: 4 cores, 8 GB RAM, 50 GB system, 200 GB data (Harbor images)
  vm = {
    ip        = "192.168.1.60"
    cores     = 4
    memory    = 8192
    disk      = 50
    data_disk = 200
  }

  # Domaine pour les services (*.home.arpa recommande - RFC 8375)
  # Les enregistrements DNS doivent pointer vers l'IP de la VM tooling:
  #   pki.home.arpa      -> 192.168.1.60
  #   registry.home.arpa -> 192.168.1.60
  #   auth.home.arpa     -> 192.168.1.60
  #   traefik.home.arpa  -> 192.168.1.60
  domain_suffix = "home.arpa"

  # Activer Traefik (reverse proxy avec HTTPS auto)
  traefik_enabled = true

  # Step-ca PKI (Autorite de Certification interne)
  # Permet de generer des certificats TLS pour tous les services internes
  step_ca = {
    enabled          = true
    password         = "${STEP_CA_PASSWORD:?required}"  # Mot de passe CA (min 8 chars)
    provisioner_name = "acme"                           # Nom du provisioner ACME
    cert_duration    = "720h"                           # Duree des certificats (30 jours)
  }

  # Harbor Registry (Registre Docker prive)
  # Stocke les images Docker avec scan de vulnerabilites Trivy
  harbor = {
    enabled        = true
    admin_password = "${HARBOR_ADMIN_PASSWORD:?required}"  # Mot de passe admin (min 8 chars)
    trivy_enabled  = true                                   # Scanner de vulnerabilites
  }

  # Authentik SSO (Single Sign-On)
  # Authentification centralisee pour Grafana, Harbor, etc.
  authentik = {
    enabled            = true
    secret_key         = "${AUTHENTIK_SECRET_KEY:?required}"         # Cle secrete (min 50 chars)
    bootstrap_password = "${AUTHENTIK_BOOTSTRAP_PASSWORD:?required}" # Mot de passe admin initial
    bootstrap_email    = "admin@home.arpa"                           # Email admin
  }
}
