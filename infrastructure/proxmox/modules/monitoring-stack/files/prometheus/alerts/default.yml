# =============================================================================
# Prometheus Alert Rules - Default
# =============================================================================

groups:
  # ---------------------------------------------------------------------------
  # Node Alerts - Host system monitoring
  # ---------------------------------------------------------------------------
  - name: node-alerts
    rules:
      - alert: HostDown
        expr: up{job="node-exporter"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Host {{ $labels.instance }} is down"
          description: "Host {{ $labels.instance }} has been unreachable for more than 2 minutes."

      - alert: HighCpuUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is above 85% (current: {{ $value | printf \"%.1f\" }}%)"

      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is above 85% (current: {{ $value | printf \"%.1f\" }}%)"

      - alert: HighDiskUsage
        expr: (1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|overlay"} / node_filesystem_size_bytes{fstype!~"tmpfs|overlay"})) * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High disk usage on {{ $labels.instance }}"
          description: "Disk usage on {{ $labels.mountpoint }} is above 85% (current: {{ $value | printf \"%.1f\" }}%)"

      - alert: DiskAlmostFull
        expr: (1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|overlay"} / node_filesystem_size_bytes{fstype!~"tmpfs|overlay"})) * 100 > 95
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Disk almost full on {{ $labels.instance }}"
          description: "Disk usage on {{ $labels.mountpoint }} is above 95% (current: {{ $value | printf \"%.1f\" }}%)"

      - alert: NodeFilesystemAlmostOutOfInodes
        expr: (1 - (node_filesystem_files_free{fstype!~"tmpfs|overlay"} / node_filesystem_files{fstype!~"tmpfs|overlay"})) * 100 > 90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Filesystem almost out of inodes on {{ $labels.instance }}"
          description: "Inode usage on {{ $labels.mountpoint }} is above 90% (current: {{ $value | printf \"%.1f\" }}%)"

      - alert: SystemdServiceFailed
        expr: node_systemd_unit_state{state="failed"} == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Systemd service failed on {{ $labels.instance }}"
          description: "Service {{ $labels.name }} has been in failed state for more than 5 minutes on {{ $labels.instance }}."

      - alert: HighLoadAverage
        expr: node_load15 / count without(cpu, mode) (node_cpu_seconds_total{mode="idle"}) > 2
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High load average on {{ $labels.instance }}"
          description: "Load average (15m) is more than 2x the number of CPUs (current: {{ $value | printf \"%.2f\" }})"

      - alert: HighNetworkErrors
        expr: rate(node_network_receive_errs_total[5m]) + rate(node_network_transmit_errs_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High network errors on {{ $labels.instance }}"
          description: "Network interface {{ $labels.device }} has more than 10 errors/s (current: {{ $value | printf \"%.1f\" }}/s)"

  # ---------------------------------------------------------------------------
  # Proxmox VE Alerts
  # ---------------------------------------------------------------------------
  - name: proxmox-alerts
    rules:
      - alert: ProxmoxNodeDown
        expr: pve_up{id=~"node/.*"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Proxmox node {{ $labels.node }} is down"
          description: "Cannot reach Proxmox node {{ $labels.node }} for more than 2 minutes."

      - alert: ProxmoxVMStopped
        expr: pve_guest_info{status="stopped"} == 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "VM {{ $labels.name }} is stopped"
          description: "VM {{ $labels.name }} on {{ $labels.node }} has been stopped for more than 10 minutes."

      - alert: ProxmoxHighCpuUsage
        expr: pve_cpu_usage_ratio * 100 > 90
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on Proxmox {{ $labels.node }}"
          description: "Proxmox node CPU usage is above 90% (current: {{ $value | printf \"%.1f\" }}%)"

      - alert: ProxmoxHighMemoryUsage
        expr: (pve_memory_usage_bytes / pve_memory_size_bytes) * 100 > 90
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on Proxmox {{ $labels.node }}"
          description: "Proxmox node memory usage is above 90% (current: {{ $value | printf \"%.1f\" }}%)"

      - alert: ProxmoxStorageAlmostFull
        expr: (pve_disk_usage_bytes{id=~"storage/.*"} / pve_disk_size_bytes{id=~"storage/.*"}) * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Storage almost full on {{ $labels.node }}"
          description: "Storage usage is above 85% (current: {{ $value | printf \"%.1f\" }}%)"

  # ---------------------------------------------------------------------------
  # Prometheus self-monitoring
  # ---------------------------------------------------------------------------
  - name: prometheus-alerts
    rules:
      - alert: PrometheusTargetMissing
        expr: up == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus target missing: {{ $labels.job }}"
          description: "Target {{ $labels.instance }} in job {{ $labels.job }} is down."

      - alert: PrometheusConfigReloadFailed
        expr: prometheus_config_last_reload_successful == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Prometheus config reload failed"
          description: "Prometheus configuration reload has failed."

      - alert: PrometheusRuleEvaluationFailures
        expr: increase(prometheus_rule_evaluation_failures_total[5m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus rule evaluation failures"
          description: "Prometheus has encountered {{ $value | printf \"%.0f\" }} rule evaluation failure(s) in the last 5 minutes."

  # ---------------------------------------------------------------------------
  # Backup Alerts - vzdump monitoring (pve-exporter >= 3.7.0)
  # ---------------------------------------------------------------------------
  - name: backup-alerts
    rules:
      - alert: GuestsNotBackedUp
        expr: pve_backup_not_enabled_total > 0
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "{{ $value }} guest(s) not covered by any backup job"
          description: "There are {{ $value }} VMs/CTs without backup enabled. Check pve_backup_not_enabled_info for details."

      - alert: BackupStorageAlmostFull
        expr: (pve_disk_usage_bytes{id=~"storage/.*", content=~".*backup.*"} / pve_disk_size_bytes{id=~"storage/.*", content=~".*backup.*"}) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Backup storage almost full on {{ $labels.node }}"
          description: "Backup storage usage is above 80% (current: {{ $value | printf \"%.1f\" }}%) on {{ $labels.node }}."

      - alert: BackupStorageCritical
        expr: (pve_disk_usage_bytes{id=~"storage/.*", content=~".*backup.*"} / pve_disk_size_bytes{id=~"storage/.*", content=~".*backup.*"}) * 100 > 95
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Backup storage critically full on {{ $labels.node }}"
          description: "Backup storage usage is above 95% (current: {{ $value | printf \"%.1f\" }}%) on {{ $labels.node }}. Backups will fail."

  # ---------------------------------------------------------------------------
  # Drift Detection Alerts - Infrastructure conformity
  # ---------------------------------------------------------------------------
  - name: drift-alerts
    rules:
      - alert: DriftDetected
        expr: pve_drift_status == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Infrastructure drift detected in {{ $labels.env }}"
          description: "{{ $value }} resource(s) have drifted from Terraform state in environment {{ $labels.env }}. Run 'terraform plan' to review changes."

      - alert: DriftCheckFailed
        expr: pve_drift_status == 2
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Drift check failed for {{ $labels.env }}"
          description: "The drift detection script failed for environment {{ $labels.env }}. Check logs in /var/log/pve-drift/."

      - alert: DriftCheckStale
        expr: time() - pve_drift_last_check_timestamp > 172800
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "Drift check stale for {{ $labels.env }}"
          description: "No drift check has run for environment {{ $labels.env }} in over 48 hours."

  # ---------------------------------------------------------------------------
  # Health Check Alerts - Infrastructure health monitoring
  # ---------------------------------------------------------------------------
  - name: health-alerts
    rules:
      - alert: InfraHealthCheckFailed
        expr: pve_health_status == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Health check failed for {{ $labels.component }} in {{ $labels.env }}"
          description: "Component {{ $labels.component }} ({{ $labels.type }}) is unhealthy in environment {{ $labels.env }}."

      - alert: HealthCheckStale
        expr: time() - pve_health_last_check_timestamp > 28800
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "Health check stale for {{ $labels.env }}"
          description: "No health check has run for environment {{ $labels.env }} in over 8 hours."

  # ---------------------------------------------------------------------------
  # VM Lifecycle Alerts - Expiration, snapshots, updates
  # ---------------------------------------------------------------------------
  - name: lifecycle-alerts
    rules:
      - alert: LabVMExpired
        expr: pve_lab_expired_total > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "{{ $value }} lab VM(s) expired and stopped"
          description: "The expiration check found {{ $value }} VM(s)/LXC(s) past their expiration date and shut them down."

      - alert: SnapshotOlderThanWeek
        expr: pve_snapshot_cleanup_deleted_total > 0
        for: 1m
        labels:
          severity: info
        annotations:
          summary: "{{ $value }} old snapshot(s) cleaned up"
          description: "The snapshot cleanup removed {{ $value }} snapshot(s) older than the configured retention period."

      - alert: VMRebootRequired
        expr: node_reboot_required == 1
        for: 24h
        labels:
          severity: warning
        annotations:
          summary: "Reboot required on {{ $labels.instance }}"
          description: "Host {{ $labels.instance }} requires a reboot to apply security updates. Pending for over 24 hours."
