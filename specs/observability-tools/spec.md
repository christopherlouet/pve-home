# Sp√©cification : Outils d'Observabilit√© Compl√©mentaires

**Branche**: `feature/observability-tools`
**Date**: 2026-02-04
**Statut**: Draft
**Input**: Top 3 outils recommand√©s pour homelab : Traefik, Loki, Uptime Kuma - d√©ploy√©s sur pve-mon

---

## R√©sum√©

Enrichir la stack d'observabilit√© du homelab avec trois outils compl√©mentaires :
- Un point d'entr√©e centralis√© pour acc√©der √† tous les services avec des URLs lisibles et HTTPS automatique
- Une centralisation des logs pour investiguer les probl√®mes sans se connecter √† chaque machine
- Une surveillance de la disponibilit√© des services avec alertes visuelles

Ces outils s'int√®grent √† la VM monitoring-stack existante sur pve-mon (192.168.1.51) pour centraliser toute l'observabilit√©.

---

## User Stories (prioritis√©es)

### US1 - Acc√®s simplifi√© aux services (Priorit√©: P1) üéØ MVP

**En tant que** administrateur du homelab
**Je veux** acc√©der √† mes services via des noms lisibles (ex: grafana.home.lan)
**Afin de** ne plus avoir √† m√©moriser les ports et IPs de chaque service

**Pourquoi P1**: Sans point d'entr√©e centralis√©, chaque nouveau service n√©cessite de retenir une IP:port diff√©rente. C'est la base pour rendre le homelab utilisable au quotidien.

**Test ind√©pendant**: Depuis un navigateur sur le r√©seau local, acc√©der √† `grafana.home.lan` et voir l'interface Grafana.

**Crit√®res d'acceptation**:

1. **√âtant donn√©** un service Grafana accessible sur le port 3000, **Quand** j'acc√®de √† `grafana.home.lan`, **Alors** je suis redirig√© vers l'interface Grafana
2. **√âtant donn√©** un service Prometheus sur le port 9090, **Quand** j'acc√®de √† `prometheus.home.lan`, **Alors** je vois l'interface Prometheus
3. **√âtant donn√©** un service Alertmanager sur le port 9093, **Quand** j'acc√®de √† `alertmanager.home.lan`, **Alors** je vois l'interface Alertmanager
4. **√âtant donn√©** une URL inconnue, **Quand** j'acc√®de √† `unknown.home.lan`, **Alors** je vois une page d'erreur claire indiquant que le service n'existe pas

---

### US2 - Centralisation des logs (Priorit√©: P1) üéØ MVP

**En tant que** administrateur du homelab
**Je veux** consulter les logs de toutes mes VMs depuis un seul endroit
**Afin de** diagnostiquer les probl√®mes sans me connecter en SSH √† chaque machine

**Pourquoi P1**: Actuellement, pour investiguer un probl√®me, il faut se connecter en SSH √† chaque VM et parser les logs manuellement. Centraliser les logs est essentiel pour le diagnostic rapide.

**Test ind√©pendant**: Depuis Grafana, visualiser les logs de toutes les VMs (monitoring-stack, prod-alloc-budget, prod-alloc-ia, prod-blog-the).

**Crit√®res d'acceptation**:

1. **√âtant donn√©** un service qui √©crit dans ses logs, **Quand** j'ouvre la section Logs dans Grafana, **Alors** je vois les logs de ce service avec timestamp et niveau de log
2. **√âtant donn√©** un probl√®me sur une VM, **Quand** je filtre par hostname dans Grafana, **Alors** je vois uniquement les logs de cette VM
3. **√âtant donn√©** un message d'erreur sp√©cifique, **Quand** je recherche ce texte dans Grafana, **Alors** je trouve toutes les occurrences avec leur contexte
4. **√âtant donn√©** des logs g√©n√©r√©s il y a 7 jours, **Quand** je consulte cette p√©riode, **Alors** les logs sont toujours disponibles

---

### US3 - Surveillance de disponibilit√© (Priorit√©: P2)

**En tant que** administrateur du homelab
**Je veux** voir en un coup d'oeil si mes services sont en ligne
**Afin de** d√©tecter les pannes avant qu'elles n'impactent mon usage

**Pourquoi P2**: Les alertes Telegram existantes notifient d√©j√† des pannes, mais un tableau de bord visuel de disponibilit√© apporte une vue d'ensemble imm√©diate.

**Test ind√©pendant**: Acc√©der √† une page de statut montrant tous les services avec leur √©tat actuel.

**Crit√®res d'acceptation**:

1. **√âtant donn√©** un service en ligne, **Quand** j'ouvre le tableau de bord de statut, **Alors** je vois un indicateur vert pour ce service
2. **√âtant donn√©** un service arr√™t√©, **Quand** j'ouvre le tableau de bord de statut, **Alors** je vois un indicateur rouge pour ce service avec la dur√©e de l'indisponibilit√©
3. **√âtant donn√©** un historique de disponibilit√©, **Quand** je consulte un service, **Alors** je vois son uptime en pourcentage sur les 30 derniers jours
4. **√âtant donn√©** un service qui devient indisponible, **Quand** cette indisponibilit√© dure plus de 2 minutes, **Alors** je re√ßois une notification

---

### US4 - Certificats HTTPS automatiques (Priorit√©: P3)

**En tant que** administrateur du homelab
**Je veux** acc√©der √† mes services en HTTPS sans avertissement du navigateur
**Afin de** s√©curiser mes connexions et √©viter les messages d'erreur de certificat

**Pourquoi P3**: Sur un r√©seau local priv√©, HTTPS n'est pas critique pour la s√©curit√©. C'est un confort pour √©viter les avertissements navigateur.

**Test ind√©pendant**: Acc√©der √† `https://grafana.home.lan` sans avertissement de certificat.

**Crit√®res d'acceptation**:

1. **√âtant donn√©** un navigateur configur√© pour faire confiance √† l'autorit√© locale, **Quand** j'acc√®de √† `https://grafana.home.lan`, **Alors** la connexion est s√©curis√©e sans avertissement
2. **√âtant donn√©** un nouveau service ajout√©, **Quand** son nom est configur√©, **Alors** un certificat est automatiquement g√©n√©r√© pour lui

---

## Cas Limites (Edge Cases)

- Que se passe-t-il quand le collecteur de logs n'arrive pas √† joindre une VM ?
  ‚Üí Les logs de cette VM ne sont pas collect√©s mais les autres VMs ne sont pas impact√©es. Une alerte est g√©n√©r√©e.

- Comment le syst√®me g√®re-t-il une VM qui g√©n√®re √©norm√©ment de logs ?
  ‚Üí Un quota par VM limite l'ingestion pour prot√©ger l'espace disque.

- Que se passe-t-il si le service de statut lui-m√™me tombe en panne ?
  ‚Üí Les alertes Telegram existantes continuent de fonctionner car elles sont g√©r√©es par Alertmanager.

- Comportement avec un service qui r√©pond mais avec des erreurs ?
  ‚Üí Le service de statut v√©rifie le code HTTP. Un code 5xx est consid√©r√© comme une panne.

---

## Exigences Fonctionnelles

- **EF-001**: Le syst√®me DOIT router les requ√™tes vers le bon service en fonction du nom de domaine
- **EF-002**: Le syst√®me DOIT collecter les logs de toutes les VMs du homelab (monitoring-stack, prod-alloc-budget, prod-alloc-ia, prod-blog-the)
- **EF-003**: Le syst√®me DOIT permettre de rechercher dans les logs par texte, service, niveau de log et p√©riode
- **EF-004**: Le syst√®me DOIT v√©rifier p√©riodiquement la disponibilit√© des services configur√©s
- **EF-005**: Le syst√®me DOIT conserver les logs pendant au moins 7 jours
- **EF-006**: Le syst√®me DOIT conserver l'historique de disponibilit√© pendant au moins 30 jours
- **EF-007**: Le syst√®me DOIT s'int√©grer avec les notifications Telegram existantes
- **EF-008**: Le syst√®me DOIT d√©ployer un agent de collecte de logs sur chaque VM de production

---

## Entit√©s Cl√©s

| Entit√© | Ce qu'elle repr√©sente | Attributs cl√©s | Relations |
|--------|----------------------|----------------|-----------|
| Service | Un service supervis√© | nom, url, port, domaine | Logs, Statuts |
| Log | Une entr√©e de journal | timestamp, message, niveau, source | Service |
| Statut | Un √©tat de disponibilit√© | horodatage, dur√©e, r√©sultat | Service |
| Route | Une redirection vers un service | domaine, cible | Service |

---

## Crit√®res de Succ√®s (mesurables)

- **CS-001**: Acc√®s √† n'importe quel service existant via son nom en moins de 2 secondes
- **CS-002**: Recherche dans les logs de la derni√®re heure retourne les r√©sultats en moins de 5 secondes
- **CS-003**: Temps entre une panne et sa d√©tection inf√©rieur √† 2 minutes
- **CS-004**: Uptime du syst√®me de monitoring lui-m√™me sup√©rieur √† 99.5%
- **CS-005**: Espace disque utilis√© par les logs inf√©rieur √† 10 GB sur 7 jours

---

## Hors Scope (explicitement exclus)

- Authentification centralis√©e (SSO) pour les services - future it√©ration
- Certificats publics Let's Encrypt (n√©cessite domaine public) - uniquement certificats locaux auto-sign√©s
- Haute disponibilit√© de la stack monitoring - un seul noeud suffit pour un homelab
- Collecte des logs Proxmox VE eux-m√™mes - hors p√©rim√®tre initial
- Collecte des logs des applications dans les conteneurs Docker (uniquement logs syst√®me et Docker daemon)

---

## Hypoth√®ses et D√©pendances

### Hypoth√®ses

- Le r√©seau local utilise un DNS qui peut r√©soudre `*.home.lan` vers la VM monitoring (ou configuration hosts locale)
- Les ressources de la VM monitoring (4 GB RAM, 30+50 GB disk) sont suffisantes pour les nouveaux services
- Les navigateurs sur le r√©seau accepteront les certificats auto-sign√©s apr√®s configuration initiale

### D√©pendances

- Stack monitoring existante (Prometheus, Grafana, Alertmanager) sur pve-mon (192.168.1.51)
- Docker Compose d√©j√† install√© et fonctionnel sur la VM monitoring
- Module Terraform monitoring-stack existant pour √©tendre la configuration
- Acc√®s SSH depuis monitoring vers les VMs prod pour d√©ployer les agents de collecte
- Module VM existant pour ajouter l'agent de logs aux VMs de production

---

## Points de Clarification

- ~~[CLARIFICATION N√âCESSAIRE]: Domaine √† utiliser pour les URLs locales~~ ‚Üí **R√©solu : `*.home.lan`**
- ~~[CLARIFICATION N√âCESSAIRE]: Les VMs de production doivent-elles envoyer leurs logs~~ ‚Üí **R√©solu : Oui, toutes les VMs (monitoring + prod)**

---

## Architecture Recommand√©e : pve-mon

**Pourquoi d√©ployer sur pve-mon ?**

| Crit√®re | pve-mon (recommand√©) | pve-prod |
|---------|---------------------|----------|
| Coh√©rence | Centralise toute l'observabilit√© | M√©lange workloads et infra |
| Impact | Aucun impact sur les VMs applicatives | Risque de surcharge |
| Maintenance | Une seule VM √† maintenir pour l'infra | Dispersion des outils |
| Int√©gration Grafana | D√©j√† pr√©sent, ajout de datasources | Duplication de Grafana |
| R√©seau | Acc√®s √† toutes les IPs du homelab | Idem |

**Ressources actuelles de monitoring-stack :**
- 2 cores, 4 GB RAM, 30 GB syst√®me + 50 GB donn√©es
- Services actuels : Prometheus, Grafana, Alertmanager, Node Exporter, PVE Exporter

**Estimation pour les nouveaux services :**
- Reverse proxy : ~50 MB RAM, CPU n√©gligeable
- Collecteur de logs : ~200 MB RAM, 10 GB stockage additionnel
- Surveillance disponibilit√© : ~100 MB RAM, stockage n√©gligeable

**Total estim√© : ~350 MB RAM suppl√©mentaire** ‚Üí Largement dans les capacit√©s actuelles.

---

## Checklist de validation

### Compl√©tude
- [x] Toutes les user stories ont des crit√®res d'acceptation
- [x] Aucun d√©tail d'impl√©mentation (langages, frameworks, APIs)
- [x] Focus sur la valeur utilisateur et les besoins m√©tier
- [x] Compr√©hensible par un non-d√©veloppeur

### Exigences
- [x] Pas de marqueur [CLARIFICATION N√âCESSAIRE] non r√©solu (tous r√©solus)
- [x] Exigences testables et non ambigu√´s
- [x] Crit√®res de succ√®s mesurables
- [x] Crit√®res technology-agnostic

### Pr√™t pour planification
- [x] Toutes les exigences fonctionnelles ont des crit√®res clairs
- [x] User stories couvrent les flux principaux
- [x] La feature apporte une valeur mesurable

---

**Version**: 1.0 | **Cr√©√©**: 2026-02-04 | **Derni√®re modification**: 2026-02-04
